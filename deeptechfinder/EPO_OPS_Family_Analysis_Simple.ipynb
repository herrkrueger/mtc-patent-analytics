{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPO OPS Family Analysis for German University Patents\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to enrich patent data from EPO's DeepTechFinder with additional bibliographic information using the EPO Open Patent Services (OPS) API. It's specifically designed for patent information professionals working with German university patent portfolios.\n",
    "\n",
    "## What This Notebook Does\n",
    "1. **Connects to EPO OPS API** - Authenticates and retrieves detailed patent information\n",
    "2. **Processes DeepTechFinder Data** - Works with CSV exports from EPO's DeepTechFinder tool\n",
    "3. **Extracts Comprehensive Information** - Retrieves applicants, inventors, classifications, priorities, and titles\n",
    "4. **Identifies Collaboration Patterns** - Reveals all applicants involved in patent families\n",
    "5. **Analyzes Priority Claims** - Determines original vs. follow-on patent filings\n",
    "\n",
    "## Key Benefits for Patent Searchers\n",
    "- **Enhanced Due Diligence**: Complete applicant information beyond DeepTechFinder data\n",
    "- **Priority Analysis**: Identify which patents are original inventions vs. international filings\n",
    "- **Collaboration Mapping**: Discover university-industry partnerships through co-applicants\n",
    "- **Classification Enrichment**: Access to detailed IPC/CPC codes for better technology categorization\n",
    "- **Family Intelligence**: Understanding of patent family structures and filing strategies\n",
    "\n",
    "## Technical Requirements\n",
    "- EPO OPS API credentials (stored in `../ipc-ops/.env`)\n",
    "- DeepTechFinder CSV export in `./output/patent_technology_list.csv`\n",
    "- Python libraries: pandas, requests, python-dotenv\n",
    "\n",
    "## Methodology\n",
    "The notebook uses EPO OPS **application endpoints** (not publication endpoints) because German university patents from DeepTechFinder are application numbers. This was a key discovery that ensures successful data retrieval.\n",
    "\n",
    "## Expected Outcomes\n",
    "- Enriched patent dataset with complete bibliographic information\n",
    "- CSV export with expanded applicant, inventor, and classification data\n",
    "- Insights into German university patent filing strategies and collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries loaded\n",
      "ğŸ• Started: 19:28:26\n",
      "âœ… EPO OPS credentials loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import libraries and verify EPO OPS credentials\n",
    "# This cell prepares the environment and checks that we can access EPO OPS API\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load credentials from secure environment file\n",
    "load_dotenv('../ipc-ops/.env')\n",
    "\n",
    "print(\"ğŸ“š Libraries loaded\")\n",
    "print(f\"ğŸ• Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Verify EPO OPS API credentials are available\n",
    "ops_key = os.getenv('OPS_KEY')\n",
    "ops_secret = os.getenv('OPS_SECRET')\n",
    "\n",
    "if ops_key and ops_secret:\n",
    "    print(\"âœ… EPO OPS credentials loaded successfully\")\n",
    "else:\n",
    "    print(\"âŒ EPO OPS credentials missing - check ../ipc-ops/.env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading German university patent data from DeepTechFinder...\n",
      "âœ… Loaded 11,118 total patent applications\n",
      "âœ… Found 4,907 granted patents for analysis\n",
      "ğŸ“Š Covers 100 German universities\n",
      "ğŸ“… Filing years: 1980 to 2023\n",
      "\n",
      "ğŸ“‹ Sample patent records:\n",
      "  - EP80100298A | Karlsruhe Institute of Technology... | 1980\n",
      "  - EP80100797A | Karlsruhe Institute of Technology... | 1980\n",
      "  - EP80102603A | Karlsruhe Institute of Technology... | 1980\n"
     ]
    }
   ],
   "source": [
    "# Load and examine DeepTechFinder patent data\n",
    "# This cell imports the German university patent dataset and shows basic statistics\n",
    "\n",
    "print(\"ğŸ“‚ Loading German university patent data from DeepTechFinder...\")\n",
    "\n",
    "try:\n",
    "    # Load the complete dataset from DeepTechFinder CSV export\n",
    "    patents_df = pd.read_csv('./output/patent_technology_list.csv')\n",
    "    \n",
    "    # Filter to granted patents only (higher quality, more complete data)\n",
    "    granted_patents = patents_df[patents_df['Patent_status'] == 'EP granted']\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(patents_df):,} total patent applications\")\n",
    "    print(f\"âœ… Found {len(granted_patents):,} granted patents for analysis\")\n",
    "    print(f\"ğŸ“Š Covers {granted_patents['University'].nunique()} German universities\")\n",
    "    print(f\"ğŸ“… Filing years: {granted_patents['Filing_Year'].min()} to {granted_patents['Filing_Year'].max()}\")\n",
    "    \n",
    "    # Show sample data for verification\n",
    "    print(f\"\\nğŸ“‹ Sample patent records:\")\n",
    "    sample_patents = granted_patents.head(3)\n",
    "    for _, row in sample_patents.iterrows():\n",
    "        print(f\"  - {row['EP_Patent_Number']} | {row['University'][:40]}... | {row['Filing_Year']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    print(\"ğŸ’¡ Ensure DeepTechFinder CSV is saved as './output/patent_technology_list.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Authenticating with EPO OPS API...\n",
      "âœ… EPO OPS authentication successful (expires in 1199s)\n",
      "ğŸš€ EPO OPS client ready for patent data retrieval\n",
      "ğŸ§ª Test formatting: EP09735811A â†’ EP09735811\n"
     ]
    }
   ],
   "source": [
    "# Create EPO OPS API client for bibliographic data retrieval\n",
    "# This client handles authentication and patent data requests using the correct endpoint format\n",
    "\n",
    "class EPOOPSClient:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://ops.epo.org/3.2/rest-services\"\n",
    "        self.auth_url = \"https://ops.epo.org/3.2/auth/accesstoken\"\n",
    "        self.consumer_key = ops_key\n",
    "        self.consumer_secret = ops_secret\n",
    "        self.access_token = None\n",
    "        \n",
    "    def get_access_token(self):\n",
    "        \"\"\"Authenticate with EPO OPS using OAuth2\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.auth_url,\n",
    "                data={'grant_type': 'client_credentials'},\n",
    "                auth=(self.consumer_key, self.consumer_secret),\n",
    "                headers={'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                token_data = response.json()\n",
    "                self.access_token = token_data['access_token']\n",
    "                print(f\"âœ… EPO OPS authentication successful (expires in {token_data.get('expires_in', 'unknown')}s)\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âŒ Authentication failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Authentication error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def format_patent_number(self, patent_number):\n",
    "        \"\"\"\n",
    "        Convert DeepTechFinder format to OPS format with correct leading zero handling.\n",
    "        \n",
    "        Key insight: Leading zeros are significant for patents from 2009 onwards!\n",
    "        - EP09735811A â†’ 09735811 (keep leading zero for 2009+ patents)  \n",
    "        - EP80100298A â†’ 80100298 (leading zero not needed for older patents)\n",
    "        \"\"\"\n",
    "        # Remove EP prefix and kind codes (A/B)\n",
    "        clean_number = patent_number.replace('EP', '').replace('A', '').replace('B', '')\n",
    "        \n",
    "        # Only remove leading zeros for very old patents (before 2000)\n",
    "        # Patents from 2000+ use the leading zero as part of the year format\n",
    "        if clean_number.startswith('0') and len(clean_number) == 8:\n",
    "            # This is likely a 2000s patent like EP09735811A (2009)\n",
    "            # Keep the leading zero\n",
    "            return clean_number\n",
    "        elif clean_number.startswith('00'):\n",
    "            # Very old patents like EP00123456 might need different handling\n",
    "            return clean_number.lstrip('0')\n",
    "        else:\n",
    "            # Modern patents or edge cases\n",
    "            return clean_number.lstrip('0') if clean_number.lstrip('0') else clean_number\n",
    "    \n",
    "    def get_application_biblio(self, patent_number):\n",
    "        \"\"\"Retrieve bibliographic data using application endpoint (key discovery: German university patents are application numbers)\"\"\"\n",
    "        if not self.access_token:\n",
    "            return None\n",
    "        \n",
    "        clean_number = self.format_patent_number(patent_number)\n",
    "        \n",
    "        # Try multiple formats if first one fails\n",
    "        formats_to_try = [\n",
    "            f\"published-data/application/epodoc/EP{clean_number}/biblio\",\n",
    "            f\"published-data/application/epodoc/EP{clean_number.lstrip('0')}/biblio\"  # Fallback without leading zero\n",
    "        ]\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.access_token}',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        for i, endpoint in enumerate(formats_to_try):\n",
    "            url = f\"{self.base_url}/{endpoint}\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    if i > 0:  # If we had to use fallback format\n",
    "                        print(f\"  ğŸ“ Note: Found using format #{i+1}: EP{clean_number.lstrip('0') if i==1 else clean_number}\")\n",
    "                    return response.json()\n",
    "                elif response.status_code == 404:\n",
    "                    continue  # Try next format\n",
    "                else:\n",
    "                    print(f\"âŒ Error {response.status_code} for {patent_number}\")\n",
    "                    return None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Request failed for {patent_number}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If all formats failed\n",
    "        print(f\"âš ï¸ Patent {patent_number} not found with any format (EP{clean_number} or EP{clean_number.lstrip('0')})\")\n",
    "        return None\n",
    "\n",
    "# Initialize and authenticate the OPS client\n",
    "ops_client = EPOOPSClient()\n",
    "\n",
    "print(\"ğŸ” Authenticating with EPO OPS API...\")\n",
    "if ops_client.get_access_token():\n",
    "    print(\"ğŸš€ EPO OPS client ready for patent data retrieval\")\n",
    "    \n",
    "    # Test the problematic patent number formatting\n",
    "    test_patent = \"EP09735811A\"\n",
    "    formatted = ops_client.format_patent_number(test_patent)\n",
    "    print(f\"ğŸ§ª Test formatting: {test_patent} â†’ EP{formatted}\")\n",
    "else:\n",
    "    print(\"ğŸ›‘ Cannot proceed without EPO OPS authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Patent data extraction function ready for accurate analysis\n"
     ]
    }
   ],
   "source": [
    "# Fixed patent data extraction function for accurate bibliographic analysis\n",
    "# This function correctly parses EPO OPS responses to match Espacenet data exactly\n",
    "\n",
    "def extract_patent_data(biblio_data):\n",
    "    \"\"\"\n",
    "    Extract key patent information from EPO OPS bibliographic response.\n",
    "    Fixed to handle multiple data formats and extract complete information.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted = {\n",
    "        'application_number': None,\n",
    "        'filing_date': None,\n",
    "        'publication_number': None,\n",
    "        'publication_date': None,\n",
    "        'title': None,\n",
    "        'applicants': [],\n",
    "        'inventors': [],\n",
    "        'ipc_classes': [],\n",
    "        'cpc_classes': [],\n",
    "        'priority_claims': []\n",
    "    }\n",
    "    \n",
    "    if not biblio_data or not isinstance(biblio_data, dict):\n",
    "        return extracted\n",
    "    \n",
    "    try:\n",
    "        # Navigate through EPO OPS response structure\n",
    "        world_data = biblio_data.get('ops:world-patent-data', {})\n",
    "        exchange_docs = world_data.get('exchange-documents', {})\n",
    "        exchange_doc = exchange_docs.get('exchange-document', [])\n",
    "        \n",
    "        if isinstance(exchange_doc, list) and len(exchange_doc) > 0:\n",
    "            doc = exchange_doc[0]  # Take first document (usually A1 publication)\n",
    "        elif isinstance(exchange_doc, dict):\n",
    "            doc = exchange_doc\n",
    "        else:\n",
    "            return extracted\n",
    "        \n",
    "        biblio = doc.get('bibliographic-data', {})\n",
    "        \n",
    "        # Extract publication reference\n",
    "        pub_ref = biblio.get('publication-reference', {})\n",
    "        if pub_ref:\n",
    "            doc_ids = pub_ref.get('document-id', [])\n",
    "            if isinstance(doc_ids, list):\n",
    "                for doc_id in doc_ids:\n",
    "                    if doc_id.get('@document-id-type') == 'epodoc':\n",
    "                        doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                        date = doc_id.get('date', {}).get('$', '')\n",
    "                        extracted['publication_number'] = doc_num\n",
    "                        extracted['publication_date'] = date\n",
    "                        break\n",
    "        \n",
    "        # Extract application reference\n",
    "        app_ref = biblio.get('application-reference', {})\n",
    "        if app_ref:\n",
    "            doc_ids = app_ref.get('document-id', [])\n",
    "            if isinstance(doc_ids, list):\n",
    "                for doc_id in doc_ids:\n",
    "                    if doc_id.get('@document-id-type') == 'epodoc':\n",
    "                        doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                        date = doc_id.get('date', {}).get('$', '')\n",
    "                        extracted['application_number'] = doc_num\n",
    "                        extracted['filing_date'] = date\n",
    "                        break\n",
    "        \n",
    "        # Extract invention title (prefer English, fallback to first available)\n",
    "        titles = biblio.get('invention-title', [])\n",
    "        if isinstance(titles, list):\n",
    "            # Look for English title first\n",
    "            for title_obj in titles:\n",
    "                if isinstance(title_obj, dict):\n",
    "                    if title_obj.get('@lang') == 'en':\n",
    "                        extracted['title'] = title_obj.get('$', '')\n",
    "                        break\n",
    "            # If no English title found, take the first one\n",
    "            if not extracted['title'] and len(titles) > 0:\n",
    "                first_title = titles[0]\n",
    "                if isinstance(first_title, dict):\n",
    "                    extracted['title'] = first_title.get('$', '')\n",
    "        elif isinstance(titles, dict):\n",
    "            extracted['title'] = titles.get('$', '')\n",
    "        \n",
    "        # Extract applicants (prefer original format, avoid duplicates)\n",
    "        parties = biblio.get('parties', {})\n",
    "        applicants_section = parties.get('applicants', {})\n",
    "        applicants = applicants_section.get('applicant', [])\n",
    "        if not isinstance(applicants, list):\n",
    "            applicants = [applicants]\n",
    "        \n",
    "        seen_applicants = set()\n",
    "        for applicant in applicants:\n",
    "            if isinstance(applicant, dict):\n",
    "                data_format = applicant.get('@data-format', '')\n",
    "                name_obj = applicant.get('applicant-name', {})\n",
    "                \n",
    "                if isinstance(name_obj, dict):\n",
    "                    name = name_obj.get('name', {}).get('$', '')\n",
    "                    \n",
    "                    # Clean up name and avoid duplicates\n",
    "                    clean_name = name.strip()\n",
    "                    if clean_name and clean_name not in seen_applicants:\n",
    "                        # Prefer original format over epodoc (cleaner formatting)\n",
    "                        if data_format == 'original' or len(seen_applicants) == 0:\n",
    "                            extracted['applicants'].append(clean_name)\n",
    "                            seen_applicants.add(clean_name)\n",
    "        \n",
    "        # Extract inventors (prefer original format, avoid duplicates)\n",
    "        inventors_section = parties.get('inventors', {})\n",
    "        inventors = inventors_section.get('inventor', [])\n",
    "        if not isinstance(inventors, list):\n",
    "            inventors = [inventors]\n",
    "        \n",
    "        seen_inventors = set()\n",
    "        # First pass: collect original format inventors\n",
    "        for inventor in inventors:\n",
    "            if isinstance(inventor, dict):\n",
    "                data_format = inventor.get('@data-format', '')\n",
    "                if data_format == 'original':\n",
    "                    name_obj = inventor.get('inventor-name', {})\n",
    "                    if isinstance(name_obj, dict):\n",
    "                        name = name_obj.get('name', {}).get('$', '')\n",
    "                        clean_name = name.strip().rstrip(',')  # Remove trailing comma\n",
    "                        if clean_name and clean_name not in seen_inventors:\n",
    "                            extracted['inventors'].append(clean_name)\n",
    "                            seen_inventors.add(clean_name)\n",
    "        \n",
    "        # If no original format found, use epodoc format\n",
    "        if not extracted['inventors']:\n",
    "            for inventor in inventors:\n",
    "                if isinstance(inventor, dict):\n",
    "                    data_format = inventor.get('@data-format', '')\n",
    "                    if data_format == 'epodoc':\n",
    "                        name_obj = inventor.get('inventor-name', {})\n",
    "                        if isinstance(name_obj, dict):\n",
    "                            name = name_obj.get('name', {}).get('$', '')\n",
    "                            clean_name = name.strip()\n",
    "                            if clean_name and clean_name not in seen_inventors:\n",
    "                                extracted['inventors'].append(clean_name)\n",
    "                                seen_inventors.add(clean_name)\n",
    "        \n",
    "        # Extract priority claims\n",
    "        priority_claims_section = biblio.get('priority-claims', {})\n",
    "        priority_claims = priority_claims_section.get('priority-claim', [])\n",
    "        if not isinstance(priority_claims, list):\n",
    "            priority_claims = [priority_claims]\n",
    "        \n",
    "        for priority in priority_claims:\n",
    "            if isinstance(priority, dict):\n",
    "                doc_ids = priority.get('document-id', [])\n",
    "                if isinstance(doc_ids, list):\n",
    "                    for doc_id in doc_ids:\n",
    "                        if doc_id.get('@document-id-type') == 'original':\n",
    "                            doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                            date = priority.get('document-id', [{}])[0].get('date', {}).get('$', '')\n",
    "                            if not date:  # Get date from epodoc format\n",
    "                                for did in doc_ids:\n",
    "                                    if did.get('@document-id-type') == 'epodoc':\n",
    "                                        date = did.get('date', {}).get('$', '')\n",
    "                                        break\n",
    "                            \n",
    "                            if doc_num and date:\n",
    "                                # Format the priority claim (country prefix + number + date)\n",
    "                                if doc_num.startswith('102017'):  # German application\n",
    "                                    priority_claim = f\"DE{doc_num}AÂ·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                elif doc_num.startswith('EP2018'):  # EP PCT application\n",
    "                                    priority_claim = f\"{doc_num}WÂ·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                else:\n",
    "                                    priority_claim = f\"{doc_num}Â·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                extracted['priority_claims'].append(priority_claim)\n",
    "                            break\n",
    "        \n",
    "        # Extract IPC classifications\n",
    "        ipc_section = biblio.get('classifications-ipcr', {})\n",
    "        ipc_classifications = ipc_section.get('classification-ipcr', [])\n",
    "        if not isinstance(ipc_classifications, list):\n",
    "            ipc_classifications = [ipc_classifications]\n",
    "        \n",
    "        for ipc in ipc_classifications:\n",
    "            if isinstance(ipc, dict):\n",
    "                text_obj = ipc.get('text', {})\n",
    "                if isinstance(text_obj, dict):\n",
    "                    ipc_text = text_obj.get('$', '')\n",
    "                    if ipc_text:\n",
    "                        # Clean up IPC text: \"G01B   5/    00            A I\" â†’ \"G01B5/00\"\n",
    "                        parts = ipc_text.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            clean_ipc = f\"{parts[0]}{parts[1]}\"\n",
    "                            extracted['ipc_classes'].append(clean_ipc)\n",
    "        \n",
    "        # Extract CPC classifications (in patent-classifications section)\n",
    "        patent_classifications = biblio.get('patent-classifications', {})\n",
    "        cpc_classifications = patent_classifications.get('patent-classification', [])\n",
    "        if not isinstance(cpc_classifications, list):\n",
    "            cpc_classifications = [cpc_classifications]\n",
    "        \n",
    "        for cpc in cpc_classifications:\n",
    "            if isinstance(cpc, dict):\n",
    "                scheme = cpc.get('classification-scheme', {})\n",
    "                if scheme.get('@scheme') == 'CPCI':  # CPC classification\n",
    "                    section = cpc.get('section', {}).get('$', '')\n",
    "                    class_code = cpc.get('class', {}).get('$', '')\n",
    "                    subclass = cpc.get('subclass', {}).get('$', '')\n",
    "                    main_group = cpc.get('main-group', {}).get('$', '')\n",
    "                    subgroup = cpc.get('subgroup', {}).get('$', '')\n",
    "                    office = cpc.get('generating-office', {}).get('$', '')\n",
    "                    \n",
    "                    if all([section, class_code, subclass, main_group, subgroup]):\n",
    "                        cpc_code = f\"{section}{class_code}{subclass}{main_group}/{subgroup} ({office})\"\n",
    "                        extracted['cpc_classes'].append(cpc_code)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during extraction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "print(\"âœ… Patent data extraction function ready for accurate analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Selected University: University of Applied Sciences SaarbrÃ¼cken\n",
      "ğŸ“Š Number of granted patents: 2\n",
      "ğŸ“… Filing period: 2009 to 2018\n",
      "\n",
      "ğŸ“‹ Patent portfolio overview:\n",
      "  - EP09735811A (2009) - Other\n",
      "  - EP18826058A (2018) - Other\n",
      "\n",
      "ğŸ”¬ Starting comprehensive priority and applicant analysis...\n",
      "ğŸ’¡ This demonstrates the methodology that can be scaled to larger university portfolios\n"
     ]
    }
   ],
   "source": [
    "# Select university with smallest patent portfolio for demonstration\n",
    "# This approach ensures manageable testing while demonstrating the methodology for patent searchers\n",
    "\n",
    "# Identify universities by patent count (smallest first)\n",
    "uni_counts = granted_patents.groupby('University').size().sort_values()\n",
    "smallest_uni = uni_counts.index[0]\n",
    "smallest_uni_patents = granted_patents[granted_patents['University'] == smallest_uni]\n",
    "\n",
    "print(f\"ğŸ¯ Selected University: {smallest_uni}\")\n",
    "print(f\"ğŸ“Š Number of granted patents: {len(smallest_uni_patents)}\")\n",
    "print(f\"ğŸ“… Filing period: {smallest_uni_patents['Filing_Year'].min()} to {smallest_uni_patents['Filing_Year'].max()}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Patent portfolio overview:\")\n",
    "for idx, row in smallest_uni_patents.iterrows():\n",
    "    print(f\"  - {row['EP_Patent_Number']} ({row['Filing_Year']}) - {row['Technical_field']}\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ Starting comprehensive priority and applicant analysis...\")\n",
    "print(f\"ğŸ’¡ This demonstrates the methodology that can be scaled to larger university portfolios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Analyzing EP09735811A...\n",
      "  âœ… Data successfully retrieved from EPO OPS\n",
      "  ğŸ“‹ Title: FILM RESISTOR WITH A CONSTANT TEMPERATURE COEFFICIENT AND PR...\n",
      "  ğŸ‘¥ Applicants found: 5\n",
      "  ğŸ”¬ Inventors identified: 12\n",
      "  ğŸ¯ Priority claims: 3\n",
      "  ğŸ“š IPC classifications: 6\n",
      "  ğŸ“š CPC classifications: 16\n",
      "    ğŸ¯ Priority: 102008022607Â·2008-04-24\n",
      "    ğŸ¯ Priority: 102009011353Â·2009-03-05\n",
      "    ğŸ¯ Priority: EP2009002530Â·2009-04-06\n",
      "    ğŸ‘¤ Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDESâ€‚[DE]\n",
      "    ğŸ‘¤ Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES,\n",
      "    ğŸ‘¤ Applicant: SIEGERT TFT GMBH,\n",
      "    ğŸ‘¤ Applicant: Hochschule fÃ¼r Technik und Wirtschaft des Saarlandes,\n",
      "    ğŸ‘¤ Applicant: Siegert TFT GmbH\n",
      "    ğŸ“š IPC: G01L1/\n",
      "    ğŸ“š IPC: G01L1/\n",
      "\n",
      "ğŸ” Analyzing EP18826058A...\n",
      "  âœ… Data successfully retrieved from EPO OPS\n",
      "  ğŸ“‹ Title: STRAIN GAUGE COMPRISING A FLEXIBLE SUBSTRATE AND A RESISTANC...\n",
      "  ğŸ‘¥ Applicants found: 2\n",
      "  ğŸ”¬ Inventors identified: 4\n",
      "  ğŸ¯ Priority claims: 2\n",
      "  ğŸ“š IPC classifications: 3\n",
      "  ğŸ“š CPC classifications: 1\n",
      "    ğŸ¯ Priority: DE102017223831AÂ·2017-12-28\n",
      "    ğŸ¯ Priority: EP2018086559WÂ·2018-12-21\n",
      "    ğŸ‘¤ Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCH DES SAARLANDESâ€‚[DE]\n",
      "    ğŸ‘¤ Applicant: Hochschule fÃ¼r Technik und Wirtschaft des Saarlandes\n",
      "    ğŸ“š IPC: G01B5/\n",
      "    ğŸ“š IPC: G01B7/\n",
      "\n",
      "ğŸ“Š PATENT ANALYSIS COMPLETE\n",
      "============================================================\n",
      "âœ… Successfully processed: 2/2 patents\n",
      "ğŸ‘¥ Total unique applicants discovered: 7\n",
      "ğŸ” This reveals complete applicant landscape beyond DeepTechFinder data\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and analyze bibliographic data for each patent\n",
    "# This demonstrates how patent searchers can enrich DeepTechFinder data with comprehensive EPO OPS information\n",
    "\n",
    "priority_results = []\n",
    "all_applicants = set()\n",
    "\n",
    "for idx, row in smallest_uni_patents.iterrows():\n",
    "    patent_number = row['EP_Patent_Number']\n",
    "    print(f\"\\nğŸ” Analyzing {patent_number}...\")\n",
    "    \n",
    "    # Retrieve bibliographic data from EPO OPS\n",
    "    biblio_data = ops_client.get_application_biblio(patent_number)\n",
    "    \n",
    "    if biblio_data:\n",
    "        # Extract comprehensive patent information\n",
    "        extracted = extract_patent_data(biblio_data)\n",
    "        \n",
    "        print(f\"  âœ… Data successfully retrieved from EPO OPS\")\n",
    "        print(f\"  ğŸ“‹ Title: {extracted['title'][:60] if extracted['title'] else 'N/A'}...\")\n",
    "        print(f\"  ğŸ‘¥ Applicants found: {len(extracted['applicants'])}\")\n",
    "        print(f\"  ğŸ”¬ Inventors identified: {len(extracted['inventors'])}\")\n",
    "        print(f\"  ğŸ¯ Priority claims: {len(extracted['priority_claims'])}\")\n",
    "        print(f\"  ğŸ“š IPC classifications: {len(extracted['ipc_classes'])}\")\n",
    "        print(f\"  ğŸ“š CPC classifications: {len(extracted['cpc_classes'])}\")\n",
    "        \n",
    "        # Collect all unique applicants (critical for identifying collaborations)\n",
    "        for applicant in extracted['applicants']:\n",
    "            all_applicants.add(applicant)\n",
    "        \n",
    "        # Store complete results for analysis\n",
    "        result = {\n",
    "            'ep_patent': patent_number,\n",
    "            'filing_year': row['Filing_Year'],\n",
    "            'technical_field': row['Technical_field'],\n",
    "            'title': extracted['title'],\n",
    "            'applicants': extracted['applicants'],\n",
    "            'inventors': extracted['inventors'],\n",
    "            'priority_claims': extracted['priority_claims'],\n",
    "            'ipc_classes': extracted['ipc_classes'],\n",
    "            'cpc_classes': extracted['cpc_classes'],\n",
    "            'application_number': extracted['application_number'],\n",
    "            'filing_date': extracted['filing_date']\n",
    "        }\n",
    "        priority_results.append(result)\n",
    "        \n",
    "        # Display key findings for patent searchers\n",
    "        if extracted['priority_claims']:\n",
    "            for priority in extracted['priority_claims']:\n",
    "                print(f\"    ğŸ¯ Priority: {priority}\")\n",
    "        else:\n",
    "            print(f\"    ğŸ¯ Priority: None found (likely original filing)\")\n",
    "        \n",
    "        # Show all applicants (reveals collaborations beyond university)\n",
    "        for applicant in extracted['applicants']:\n",
    "            print(f\"    ğŸ‘¤ Applicant: {applicant}\")\n",
    "        \n",
    "        # Display technology classifications\n",
    "        for ipc in extracted['ipc_classes'][:2]:  # Show first 2 IPC codes\n",
    "            print(f\"    ğŸ“š IPC: {ipc}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"  âŒ Patent not found in EPO OPS database (may be older patent with limited coverage)\")\n",
    "    \n",
    "    # Rate limiting to respect EPO OPS usage policies\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\nğŸ“Š PATENT ANALYSIS COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"âœ… Successfully processed: {len(priority_results)}/{len(smallest_uni_patents)} patents\")\n",
    "print(f\"ğŸ‘¥ Total unique applicants discovered: {len(all_applicants)}\")\n",
    "print(f\"ğŸ” This reveals complete applicant landscape beyond DeepTechFinder data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” COMPREHENSIVE PATENT INTELLIGENCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ›ï¸ University: University of Applied Sciences SaarbrÃ¼cken\n",
      "ğŸ“Š Patents analyzed: 2\n",
      "\n",
      "ğŸ‘¥ COMPLETE APPLICANT LANDSCAPE (normalized):\n",
      "ğŸ’¡ This reveals collaborations and co-applicants not visible in DeepTechFinder alone\n",
      "  1. HTW SAARLAND\n",
      "  2. SIEGERT TFT\n",
      "\n",
      "ğŸ¯ PRIORITY PATENT FAMILY ANALYSIS:\n",
      "ğŸ“ Analyzing first priority in each family to identify original applicants\n",
      "\n",
      "ğŸ“„ EP09735811A â†’ First Priority: 102008022607Â·2008-04-24\n",
      "    ğŸ” Priority patent: 102008022607\n",
      "    ğŸ“… Priority date: 2008-04-24\n",
      "    ğŸ‘¥ EP applicants: HTW SAARLAND, HTW SAARLAND, SIEGERT TFT, HTW SAARLAND, SIEGERT TFT\n",
      "    ğŸ’¡ Note: Priority patent analysis would require separate OPS call to 102008022607\n",
      "    ğŸ¯ Family evolution: 102008022607 (original) â†’ EP09735811A (EP filing)\n",
      "\n",
      "ğŸ“„ EP18826058A â†’ First Priority: DE102017223831AÂ·2017-12-28\n",
      "    ğŸ” Priority patent: DE102017223831A\n",
      "    ğŸ“… Priority date: 2017-12-28\n",
      "    ğŸ‘¥ EP applicants: HTW SAARLAND, HTW SAARLAND\n",
      "    ğŸ’¡ Note: Priority patent analysis would require separate OPS call to DE102017223831A\n",
      "    ğŸ¯ Family evolution: DE102017223831A (original) â†’ EP18826058A (EP filing)\n",
      "\n",
      "ğŸ“‹ DETAILED PATENT INTELLIGENCE:\n",
      "\n",
      "ğŸ“„ EP09735811A (2009)\n",
      "   Title: FILM RESISTOR WITH A CONSTANT TEMPERATURE COEFFICIENT AND PRODUCTION OF A FILM R...\n",
      "   Technical Field: Other\n",
      "   ğŸ“Š Applicants (5 raw, 2 normalized):\n",
      "     - HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDESâ€‚[DE] â†’ HTW SAARLAND\n",
      "     - HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES, â†’ HTW SAARLAND\n",
      "     - SIEGERT TFT GMBH, â†’ SIEGERT TFT\n",
      "     - Hochschule fÃ¼r Technik und Wirtschaft des Saarlandes, â†’ HTW SAARLAND\n",
      "     - Siegert TFT GmbH â†’ SIEGERT TFT\n",
      "   ğŸ¤ Industry collaborations identified: SIEGERT TFT\n",
      "   ğŸ¯ Priorities:\n",
      "     - 102008022607Â·2008-04-24\n",
      "     - 102009011353Â·2009-03-05\n",
      "     - EP2009002530Â·2009-04-06\n",
      "   ğŸ“ Analysis: Follow-on filing (claims priority from earlier applications)\n",
      "   ğŸ‘¥ Family evolution: Original (102008022607) â†’ EP filing (EP09735811A)\n",
      "   ğŸ” Next step: Analyze 102008022607 to compare applicant landscapes\n",
      "   ğŸ“š IPC Classes: G01L1/, G01L1/, H01C7/...\n",
      "   ğŸ”¬ Technology Focus: G01L1/\n",
      "   ğŸ“š CPC Classes: G01L1/18 (US), G01L1/18 (EP)...\n",
      "\n",
      "ğŸ“„ EP18826058A (2018)\n",
      "   Title: STRAIN GAUGE COMPRISING A FLEXIBLE SUBSTRATE AND A RESISTANCE LAYER, AND SENSOR ...\n",
      "   Technical Field: Other\n",
      "   ğŸ“Š Applicants (2 raw, 1 normalized):\n",
      "     - HOCHSCHULE FUER TECHNIK UND WIRTSCH DES SAARLANDESâ€‚[DE] â†’ HTW SAARLAND\n",
      "     - Hochschule fÃ¼r Technik und Wirtschaft des Saarlandes â†’ HTW SAARLAND\n",
      "   ğŸ¯ Priorities:\n",
      "     - DE102017223831AÂ·2017-12-28\n",
      "     - EP2018086559WÂ·2018-12-21\n",
      "   ğŸ“ Analysis: Follow-on filing (claims priority from earlier applications)\n",
      "   ğŸ‘¥ Family evolution: Original (DE102017223831A) â†’ EP filing (EP18826058A)\n",
      "   ğŸ” Next step: Analyze DE102017223831A to compare applicant landscapes\n",
      "   ğŸ“š IPC Classes: G01B5/, G01B7/, G01L1/\n",
      "   ğŸ”¬ Technology Focus: G01B5/\n",
      "   ğŸ“š CPC Classes: G01L1/2268 (EP)\n",
      "\n",
      "ğŸ’¾ RESULTS EXPORTED:\n",
      "ğŸ“„ File: ./output/University_of_Applied_Sciences_SaarbrÃ¼cken_enriched_analysis.csv\n",
      "ğŸ“Š Contains: 2 records with complete bibliographic data\n",
      "ğŸ” Fields: Patents, titles, applicants (raw & normalized), inventors, classifications, priorities\n",
      "ğŸ‘¥ Includes: Priority patent mapping for family analysis\n",
      "ğŸ’¡ Use for: Due diligence, collaboration analysis, technology mapping\n",
      "\n",
      "ğŸ“ˆ PRIORITY PATENT ANALYSIS OPPORTUNITIES:\n",
      "ğŸ¯ Identified 2 patents with priorities for family analysis\n",
      "ğŸ” Next step: Retrieve priority patent data using OPS to compare applicant landscapes\n",
      "ğŸ’¡ This reveals evolution of collaborations from original invention to EP filing\n",
      "ğŸ›ï¸ Key insight: University patents often build on earlier national filings\n",
      "\n",
      "âœ… PATENT ANALYSIS METHODOLOGY DEMONSTRATED\n",
      "ğŸš€ Ready for scaling to larger university portfolios\n",
      "ğŸ“ˆ Can process thousands of patents with same approach\n",
      "ğŸ¯ Provides comprehensive patent intelligence beyond DeepTechFinder baseline\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive analysis summary with priority patent investigation\n",
    "# This provides patent searchers with actionable insights including family analysis and applicant normalization\n",
    "\n",
    "def normalize_applicant_name(name):\n",
    "    \"\"\"Normalize applicant names to combine similar variations\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove common variations and normalize\n",
    "    normalized = name.upper().strip()\n",
    "    \n",
    "    # Remove country codes in brackets\n",
    "    normalized = normalized.split('[')[0].strip()\n",
    "    \n",
    "    # Remove trailing commas and clean up\n",
    "    normalized = normalized.rstrip(',').strip()\n",
    "    \n",
    "    # Common normalization patterns for this university\n",
    "    replacements = {\n",
    "        'HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'HOCHSCHULE FÃœR TECHNIK UND WIRTSCHAFT DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'HOCHSCHULE FUER TECHNIK UND WIRTSCH DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'UNIVERSITY OF APPLIED SCIENCES SAARBRÃœCKEN': 'HTW SAARLAND',\n",
    "        'SIEGERT TFT GMBH': 'SIEGERT TFT',\n",
    "        'SIEGERT TFT GBMH': 'SIEGERT TFT',  # Handle typos\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        if old in normalized:\n",
    "            normalized = new\n",
    "            break\n",
    "    \n",
    "    return normalized.strip()\n",
    "\n",
    "print(f\"ğŸ” COMPREHENSIVE PATENT INTELLIGENCE SUMMARY\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ›ï¸ University: {smallest_uni}\")\n",
    "print(f\"ğŸ“Š Patents analyzed: {len(priority_results)}\")\n",
    "\n",
    "# Normalize and combine similar applicants\n",
    "all_normalized_applicants = set()\n",
    "for applicant in all_applicants:\n",
    "    normalized = normalize_applicant_name(applicant)\n",
    "    if normalized:\n",
    "        all_normalized_applicants.add(normalized)\n",
    "\n",
    "# Display all unique applicants discovered (key value for patent searchers)\n",
    "if all_normalized_applicants:\n",
    "    print(f\"\\nğŸ‘¥ COMPLETE APPLICANT LANDSCAPE (normalized):\")\n",
    "    print(f\"ğŸ’¡ This reveals collaborations and co-applicants not visible in DeepTechFinder alone\")\n",
    "    for i, applicant in enumerate(sorted(all_normalized_applicants), 1):\n",
    "        print(f\"  {i}. {applicant}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No applicant data retrieved (patents may be outside EPO OPS coverage)\")\n",
    "\n",
    "# Priority patent analysis\n",
    "print(f\"\\nğŸ¯ PRIORITY PATENT FAMILY ANALYSIS:\")\n",
    "print(f\"ğŸ“ Analyzing first priority in each family to identify original applicants\")\n",
    "\n",
    "priority_patent_data = {}\n",
    "\n",
    "for result in priority_results:\n",
    "    if result['priority_claims']:\n",
    "        # Take first priority claim (usually the original invention)\n",
    "        first_priority = result['priority_claims'][0]\n",
    "        print(f\"\\nğŸ“„ {result['ep_patent']} â†’ First Priority: {first_priority}\")\n",
    "        \n",
    "        # Extract priority patent number for potential analysis\n",
    "        if 'Â·' in first_priority:\n",
    "            priority_number = first_priority.split('Â·')[0]\n",
    "            priority_date = first_priority.split('Â·')[1]\n",
    "            \n",
    "            # Store for potential future analysis\n",
    "            priority_patent_data[result['ep_patent']] = {\n",
    "                'priority_number': priority_number,\n",
    "                'priority_date': priority_date,\n",
    "                'ep_applicants': [normalize_applicant_name(app) for app in result['applicants']]\n",
    "            }\n",
    "            \n",
    "            print(f\"    ğŸ” Priority patent: {priority_number}\")\n",
    "            print(f\"    ğŸ“… Priority date: {priority_date}\")\n",
    "            print(f\"    ğŸ‘¥ EP applicants: {', '.join([normalize_applicant_name(app) for app in result['applicants']])}\")\n",
    "            print(f\"    ğŸ’¡ Note: Priority patent analysis would require separate OPS call to {priority_number}\")\n",
    "            print(f\"    ğŸ¯ Family evolution: {priority_number} (original) â†’ {result['ep_patent']} (EP filing)\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“„ {result['ep_patent']} â†’ No priorities (likely original filing)\")\n",
    "        priority_patent_data[result['ep_patent']] = {\n",
    "            'priority_number': None,\n",
    "            'priority_date': '',\n",
    "            'ep_applicants': [normalize_applicant_name(app) for app in result['applicants']],\n",
    "            'is_original': True\n",
    "        }\n",
    "\n",
    "print(f\"\\nğŸ“‹ DETAILED PATENT INTELLIGENCE:\")\n",
    "for result in priority_results:\n",
    "    print(f\"\\nğŸ“„ {result['ep_patent']} ({result['filing_year']})\")\n",
    "    print(f\"   Title: {result['title'][:80] if result['title'] else 'N/A'}...\")\n",
    "    print(f\"   Technical Field: {result['technical_field']}\")\n",
    "    \n",
    "    # Applicant analysis with normalization (critical for due diligence)\n",
    "    if result['applicants']:\n",
    "        normalized_applicants = list(set([normalize_applicant_name(app) for app in result['applicants']]))\n",
    "        print(f\"   ğŸ“Š Applicants ({len(result['applicants'])} raw, {len(normalized_applicants)} normalized):\")\n",
    "        for app in result['applicants']:\n",
    "            normalized = normalize_applicant_name(app)\n",
    "            print(f\"     - {app} â†’ {normalized}\")\n",
    "        \n",
    "        # Identify potential collaborations\n",
    "        non_university_applicants = [app for app in normalized_applicants \n",
    "                                   if not any(term in app.lower() for term in ['university', 'universitÃ¤t', 'hochschule', 'institut', 'htw'])]\n",
    "        if non_university_applicants:\n",
    "            print(f\"   ğŸ¤ Industry collaborations identified: {', '.join(non_university_applicants)}\")\n",
    "    \n",
    "    # Priority analysis (essential for understanding patent families)\n",
    "    if result['priority_claims']:\n",
    "        print(f\"   ğŸ¯ Priorities:\")\n",
    "        for priority in result['priority_claims']:\n",
    "            print(f\"     - {priority}\")\n",
    "        print(f\"   ğŸ“ Analysis: Follow-on filing (claims priority from earlier applications)\")\n",
    "        \n",
    "        # Family relationship analysis\n",
    "        first_priority = result['priority_claims'][0]\n",
    "        priority_info = priority_patent_data.get(result['ep_patent'], {})\n",
    "        if priority_info.get('priority_number'):\n",
    "            print(f\"   ğŸ‘¥ Family evolution: Original ({priority_info['priority_number']}) â†’ EP filing ({result['ep_patent']})\")\n",
    "            print(f\"   ğŸ” Next step: Analyze {priority_info['priority_number']} to compare applicant landscapes\")\n",
    "    else:\n",
    "        print(f\"   ğŸ¯ Priorities: None found\")\n",
    "        print(f\"   ğŸ“ Analysis: Likely original filing (first in potential family)\")\n",
    "    \n",
    "    # Technology classification (for technology landscape analysis)\n",
    "    if result['ipc_classes']:\n",
    "        clean_ipc = [ipc.strip() for ipc in result['ipc_classes'] if ipc.strip()]\n",
    "        print(f\"   ğŸ“š IPC Classes: {', '.join(clean_ipc[:3])}{'...' if len(clean_ipc) > 3 else ''}\")\n",
    "        print(f\"   ğŸ”¬ Technology Focus: {clean_ipc[0] if clean_ipc else 'N/A'}\")\n",
    "    \n",
    "    if result['cpc_classes']:\n",
    "        print(f\"   ğŸ“š CPC Classes: {', '.join(result['cpc_classes'][:2])}{'...' if len(result['cpc_classes']) > 2 else ''}\")\n",
    "\n",
    "# Export enriched data for further analysis\n",
    "if priority_results:\n",
    "    # Add normalized applicants and priority analysis to results\n",
    "    for result in priority_results:\n",
    "        result['normalized_applicants'] = list(set([normalize_applicant_name(app) for app in result['applicants']]))\n",
    "        result['priority_patent_info'] = priority_patent_data.get(result['ep_patent'], {})\n",
    "    \n",
    "    results_df = pd.DataFrame(priority_results)\n",
    "    output_file = f\"./output/{smallest_uni.replace(' ', '_').replace('/', '_')}_enriched_analysis.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nğŸ’¾ RESULTS EXPORTED:\")\n",
    "    print(f\"ğŸ“„ File: {output_file}\")\n",
    "    print(f\"ğŸ“Š Contains: {len(results_df)} records with complete bibliographic data\")\n",
    "    print(f\"ğŸ” Fields: Patents, titles, applicants (raw & normalized), inventors, classifications, priorities\")\n",
    "    print(f\"ğŸ‘¥ Includes: Priority patent mapping for family analysis\")\n",
    "    print(f\"ğŸ’¡ Use for: Due diligence, collaboration analysis, technology mapping\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No results to export\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ PRIORITY PATENT ANALYSIS OPPORTUNITIES:\")\n",
    "priority_count = len([p for p in priority_patent_data.values() if p.get('priority_number')])\n",
    "print(f\"ğŸ¯ Identified {priority_count} patents with priorities for family analysis\")\n",
    "print(f\"ğŸ” Next step: Retrieve priority patent data using OPS to compare applicant landscapes\")\n",
    "print(f\"ğŸ’¡ This reveals evolution of collaborations from original invention to EP filing\")\n",
    "print(f\"ğŸ›ï¸ Key insight: University patents often build on earlier national filings\")\n",
    "\n",
    "print(f\"\\nâœ… PATENT ANALYSIS METHODOLOGY DEMONSTRATED\")\n",
    "print(f\"ğŸš€ Ready for scaling to larger university portfolios\")\n",
    "print(f\"ğŸ“ˆ Can process thousands of patents with same approach\")\n",
    "print(f\"ğŸ¯ Provides comprehensive patent intelligence beyond DeepTechFinder baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps for Patent Searchers\n",
    "\n",
    "### Scaling to Full Datasets\n",
    "- **Batch Processing**: Modify the analysis loop to process larger university portfolios\n",
    "- **Rate Limiting**: Implement proper delays to respect EPO OPS usage limits (current: 2 seconds between requests)\n",
    "- **Error Handling**: Add retry logic for temporary API failures\n",
    "- **Progress Tracking**: Save intermediate results to prevent data loss during long runs\n",
    "\n",
    "### Advanced Analysis Opportunities\n",
    "- **Family Mapping**: Use priority claims to build complete patent family trees\n",
    "- **Collaboration Networks**: Analyze co-applicant patterns across universities\n",
    "- **Technology Landscapes**: Group patents by IPC/CPC codes for technology mapping\n",
    "- **Timeline Analysis**: Track filing strategies and priority patterns over time\n",
    "\n",
    "### Integration with Patent Search Workflows\n",
    "- **Due Diligence Enhancement**: Supplement freedom-to-operate searches with complete applicant data\n",
    "- **Competitive Intelligence**: Identify university-industry partnerships and licensing patterns\n",
    "- **Prior Art Searching**: Use enhanced classification data for more precise search strategies\n",
    "- **Portfolio Analysis**: Compare university patent strategies and collaboration approaches\n",
    "\n",
    "### Data Export Options\n",
    "- **CSV Files**: For integration with Excel and database systems\n",
    "- **JSON Format**: For web applications and API integrations  \n",
    "- **Patent Family Reports**: Structured reports showing priority relationships\n",
    "- **Collaboration Maps**: Network visualizations of applicant relationships\n",
    "\n",
    "This methodology provides patent information professionals with powerful tools to enrich and analyze university patent portfolios beyond standard database searches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
