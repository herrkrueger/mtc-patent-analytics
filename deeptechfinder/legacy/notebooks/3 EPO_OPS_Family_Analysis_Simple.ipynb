{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPO OPS Family Analysis for German University Patents\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to enrich patent data from EPO's DeepTechFinder with additional bibliographic information using the EPO Open Patent Services (OPS) API. It's specifically designed for patent information professionals working with German university patent portfolios.\n",
    "\n",
    "## What This Notebook Does\n",
    "1. **Connects to EPO OPS API** - Authenticates and retrieves detailed patent information\n",
    "2. **Processes DeepTechFinder Data** - Works with CSV exports from EPO's DeepTechFinder tool\n",
    "3. **Extracts Comprehensive Information** - Retrieves applicants, inventors, classifications, priorities, and titles\n",
    "4. **Identifies Collaboration Patterns** - Reveals all applicants involved in patent families\n",
    "5. **Analyzes Priority Claims** - Determines original vs. follow-on patent filings\n",
    "\n",
    "## Key Benefits for Patent Searchers\n",
    "- **Enhanced Due Diligence**: Complete applicant information beyond DeepTechFinder data\n",
    "- **Priority Analysis**: Identify which patents are original inventions vs. international filings\n",
    "- **Collaboration Mapping**: Discover university-industry partnerships through co-applicants\n",
    "- **Classification Enrichment**: Access to detailed IPC/CPC codes for better technology categorization\n",
    "- **Family Intelligence**: Understanding of patent family structures and filing strategies\n",
    "\n",
    "## Technical Requirements\n",
    "- EPO OPS API credentials (stored in `../ipc-ops/.env`)\n",
    "- DeepTechFinder CSV export in `./output/patent_technology_list.csv`\n",
    "- Python libraries: pandas, requests, python-dotenv\n",
    "\n",
    "## Methodology\n",
    "The notebook uses EPO OPS **application endpoints** (not publication endpoints) because German university patents from DeepTechFinder are application numbers. This was a key discovery that ensures successful data retrieval.\n",
    "\n",
    "## Expected Outcomes\n",
    "- Enriched patent dataset with complete bibliographic information\n",
    "- CSV export with expanded applicant, inventor, and classification data\n",
    "- Insights into German university patent filing strategies and collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Libraries loaded\n",
      "🕐 Started: 19:36:24\n",
      "✅ EPO OPS credentials loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import libraries and verify EPO OPS credentials\n",
    "# This cell prepares the environment and checks that we can access EPO OPS API\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load credentials from secure environment file\n",
    "load_dotenv('../ipc-ops/.env')\n",
    "\n",
    "print(\"📚 Libraries loaded\")\n",
    "print(f\"🕐 Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Verify EPO OPS API credentials are available\n",
    "ops_key = os.getenv('OPS_KEY')\n",
    "ops_secret = os.getenv('OPS_SECRET')\n",
    "\n",
    "if ops_key and ops_secret:\n",
    "    print(\"✅ EPO OPS credentials loaded successfully\")\n",
    "else:\n",
    "    print(\"❌ EPO OPS credentials missing - check ../ipc-ops/.env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading German university patent data from DeepTechFinder...\n",
      "✅ Loaded 11,118 total patent applications\n",
      "✅ Found 4,907 granted patents for analysis\n",
      "📊 Covers 100 German universities\n",
      "📅 Filing years: 1980 to 2023\n",
      "\n",
      "📋 Sample patent records:\n",
      "  - EP80100298A | Karlsruhe Institute of Technology... | 1980\n",
      "  - EP80100797A | Karlsruhe Institute of Technology... | 1980\n",
      "  - EP80102603A | Karlsruhe Institute of Technology... | 1980\n"
     ]
    }
   ],
   "source": [
    "# Load and examine DeepTechFinder patent data\n",
    "# This cell imports the German university patent dataset and shows basic statistics\n",
    "\n",
    "print(\"📂 Loading German university patent data from DeepTechFinder...\")\n",
    "\n",
    "try:\n",
    "    # Load the complete dataset from DeepTechFinder CSV export\n",
    "    patents_df = pd.read_csv('./output/patent_technology_list.csv')\n",
    "    \n",
    "    # Filter to granted patents only (higher quality, more complete data)\n",
    "    granted_patents = patents_df[patents_df['Patent_status'] == 'EP granted']\n",
    "    \n",
    "    print(f\"✅ Loaded {len(patents_df):,} total patent applications\")\n",
    "    print(f\"✅ Found {len(granted_patents):,} granted patents for analysis\")\n",
    "    print(f\"📊 Covers {granted_patents['University'].nunique()} German universities\")\n",
    "    print(f\"📅 Filing years: {granted_patents['Filing_Year'].min()} to {granted_patents['Filing_Year'].max()}\")\n",
    "    \n",
    "    # Show sample data for verification\n",
    "    print(f\"\\n📋 Sample patent records:\")\n",
    "    sample_patents = granted_patents.head(3)\n",
    "    for _, row in sample_patents.iterrows():\n",
    "        print(f\"  - {row['EP_Patent_Number']} | {row['University'][:40]}... | {row['Filing_Year']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"💡 Ensure DeepTechFinder CSV is saved as './output/patent_technology_list.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 Authenticating with EPO OPS API...\n",
      "✅ EPO OPS authentication successful (expires in 1199s)\n",
      "🚀 EPO OPS client ready for patent data retrieval\n",
      "🧪 Test formatting: EP09735811A → EP09735811\n"
     ]
    }
   ],
   "source": [
    "# Create EPO OPS API client for bibliographic data retrieval\n",
    "# This client handles authentication and patent data requests using the correct endpoint format\n",
    "\n",
    "class EPOOPSClient:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://ops.epo.org/3.2/rest-services\"\n",
    "        self.auth_url = \"https://ops.epo.org/3.2/auth/accesstoken\"\n",
    "        self.consumer_key = ops_key\n",
    "        self.consumer_secret = ops_secret\n",
    "        self.access_token = None\n",
    "        \n",
    "    def get_access_token(self):\n",
    "        \"\"\"Authenticate with EPO OPS using OAuth2\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.auth_url,\n",
    "                data={'grant_type': 'client_credentials'},\n",
    "                auth=(self.consumer_key, self.consumer_secret),\n",
    "                headers={'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                token_data = response.json()\n",
    "                self.access_token = token_data['access_token']\n",
    "                print(f\"✅ EPO OPS authentication successful (expires in {token_data.get('expires_in', 'unknown')}s)\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ Authentication failed: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Authentication error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def format_patent_number(self, patent_number):\n",
    "        \"\"\"\n",
    "        Convert DeepTechFinder format to OPS format with correct leading zero handling.\n",
    "        \n",
    "        Key insight: Leading zeros are significant for patents from 2009 onwards!\n",
    "        - EP09735811A → 09735811 (keep leading zero for 2009+ patents)  \n",
    "        - EP80100298A → 80100298 (leading zero not needed for older patents)\n",
    "        \"\"\"\n",
    "        # Remove EP prefix and kind codes (A/B)\n",
    "        clean_number = patent_number.replace('EP', '').replace('A', '').replace('B', '')\n",
    "        \n",
    "        # Only remove leading zeros for very old patents (before 2000)\n",
    "        # Patents from 2000+ use the leading zero as part of the year format\n",
    "        if clean_number.startswith('0') and len(clean_number) == 8:\n",
    "            # This is likely a 2000s patent like EP09735811A (2009)\n",
    "            # Keep the leading zero\n",
    "            return clean_number\n",
    "        elif clean_number.startswith('00'):\n",
    "            # Very old patents like EP00123456 might need different handling\n",
    "            return clean_number.lstrip('0')\n",
    "        else:\n",
    "            # Modern patents or edge cases\n",
    "            return clean_number.lstrip('0') if clean_number.lstrip('0') else clean_number\n",
    "    \n",
    "    def get_application_biblio(self, patent_number):\n",
    "        \"\"\"Retrieve bibliographic data using application endpoint (key discovery: German university patents are application numbers)\"\"\"\n",
    "        if not self.access_token:\n",
    "            return None\n",
    "        \n",
    "        clean_number = self.format_patent_number(patent_number)\n",
    "        \n",
    "        # Try multiple formats if first one fails\n",
    "        formats_to_try = [\n",
    "            f\"published-data/application/epodoc/EP{clean_number}/biblio\",\n",
    "            f\"published-data/application/epodoc/EP{clean_number.lstrip('0')}/biblio\"  # Fallback without leading zero\n",
    "        ]\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.access_token}',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        for i, endpoint in enumerate(formats_to_try):\n",
    "            url = f\"{self.base_url}/{endpoint}\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    if i > 0:  # If we had to use fallback format\n",
    "                        print(f\"  📝 Note: Found using format #{i+1}: EP{clean_number.lstrip('0') if i==1 else clean_number}\")\n",
    "                    return response.json()\n",
    "                elif response.status_code == 404:\n",
    "                    continue  # Try next format\n",
    "                else:\n",
    "                    print(f\"❌ Error {response.status_code} for {patent_number}\")\n",
    "                    return None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Request failed for {patent_number}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If all formats failed\n",
    "        print(f\"⚠️ Patent {patent_number} not found with any format (EP{clean_number} or EP{clean_number.lstrip('0')})\")\n",
    "        return None\n",
    "\n",
    "# Initialize and authenticate the OPS client\n",
    "ops_client = EPOOPSClient()\n",
    "\n",
    "print(\"🔐 Authenticating with EPO OPS API...\")\n",
    "if ops_client.get_access_token():\n",
    "    print(\"🚀 EPO OPS client ready for patent data retrieval\")\n",
    "    \n",
    "    # Test the problematic patent number formatting\n",
    "    test_patent = \"EP09735811A\"\n",
    "    formatted = ops_client.format_patent_number(test_patent)\n",
    "    print(f\"🧪 Test formatting: {test_patent} → EP{formatted}\")\n",
    "else:\n",
    "    print(\"🛑 Cannot proceed without EPO OPS authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patent data extraction function ready for accurate analysis\n"
     ]
    }
   ],
   "source": [
    "# Fixed patent data extraction function for accurate bibliographic analysis\n",
    "# This function correctly parses EPO OPS responses to match Espacenet data exactly\n",
    "\n",
    "def extract_patent_data(biblio_data):\n",
    "    \"\"\"\n",
    "    Extract key patent information from EPO OPS bibliographic response.\n",
    "    Fixed to handle multiple data formats and extract complete information.\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted = {\n",
    "        'application_number': None,\n",
    "        'filing_date': None,\n",
    "        'publication_number': None,\n",
    "        'publication_date': None,\n",
    "        'title': None,\n",
    "        'applicants': [],\n",
    "        'inventors': [],\n",
    "        'ipc_classes': [],\n",
    "        'cpc_classes': [],\n",
    "        'priority_claims': []\n",
    "    }\n",
    "    \n",
    "    if not biblio_data or not isinstance(biblio_data, dict):\n",
    "        return extracted\n",
    "    \n",
    "    try:\n",
    "        # Navigate through EPO OPS response structure\n",
    "        world_data = biblio_data.get('ops:world-patent-data', {})\n",
    "        exchange_docs = world_data.get('exchange-documents', {})\n",
    "        exchange_doc = exchange_docs.get('exchange-document', [])\n",
    "        \n",
    "        if isinstance(exchange_doc, list) and len(exchange_doc) > 0:\n",
    "            doc = exchange_doc[0]  # Take first document (usually A1 publication)\n",
    "        elif isinstance(exchange_doc, dict):\n",
    "            doc = exchange_doc\n",
    "        else:\n",
    "            return extracted\n",
    "        \n",
    "        biblio = doc.get('bibliographic-data', {})\n",
    "        \n",
    "        # Extract publication reference\n",
    "        pub_ref = biblio.get('publication-reference', {})\n",
    "        if pub_ref:\n",
    "            doc_ids = pub_ref.get('document-id', [])\n",
    "            if isinstance(doc_ids, list):\n",
    "                for doc_id in doc_ids:\n",
    "                    if doc_id.get('@document-id-type') == 'epodoc':\n",
    "                        doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                        date = doc_id.get('date', {}).get('$', '')\n",
    "                        extracted['publication_number'] = doc_num\n",
    "                        extracted['publication_date'] = date\n",
    "                        break\n",
    "        \n",
    "        # Extract application reference\n",
    "        app_ref = biblio.get('application-reference', {})\n",
    "        if app_ref:\n",
    "            doc_ids = app_ref.get('document-id', [])\n",
    "            if isinstance(doc_ids, list):\n",
    "                for doc_id in doc_ids:\n",
    "                    if doc_id.get('@document-id-type') == 'epodoc':\n",
    "                        doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                        date = doc_id.get('date', {}).get('$', '')\n",
    "                        extracted['application_number'] = doc_num\n",
    "                        extracted['filing_date'] = date\n",
    "                        break\n",
    "        \n",
    "        # Extract invention title (prefer English, fallback to first available)\n",
    "        titles = biblio.get('invention-title', [])\n",
    "        if isinstance(titles, list):\n",
    "            # Look for English title first\n",
    "            for title_obj in titles:\n",
    "                if isinstance(title_obj, dict):\n",
    "                    if title_obj.get('@lang') == 'en':\n",
    "                        extracted['title'] = title_obj.get('$', '')\n",
    "                        break\n",
    "            # If no English title found, take the first one\n",
    "            if not extracted['title'] and len(titles) > 0:\n",
    "                first_title = titles[0]\n",
    "                if isinstance(first_title, dict):\n",
    "                    extracted['title'] = first_title.get('$', '')\n",
    "        elif isinstance(titles, dict):\n",
    "            extracted['title'] = titles.get('$', '')\n",
    "        \n",
    "        # Extract applicants (prefer original format, avoid duplicates)\n",
    "        parties = biblio.get('parties', {})\n",
    "        applicants_section = parties.get('applicants', {})\n",
    "        applicants = applicants_section.get('applicant', [])\n",
    "        if not isinstance(applicants, list):\n",
    "            applicants = [applicants]\n",
    "        \n",
    "        seen_applicants = set()\n",
    "        for applicant in applicants:\n",
    "            if isinstance(applicant, dict):\n",
    "                data_format = applicant.get('@data-format', '')\n",
    "                name_obj = applicant.get('applicant-name', {})\n",
    "                \n",
    "                if isinstance(name_obj, dict):\n",
    "                    name = name_obj.get('name', {}).get('$', '')\n",
    "                    \n",
    "                    # Clean up name and avoid duplicates\n",
    "                    clean_name = name.strip()\n",
    "                    if clean_name and clean_name not in seen_applicants:\n",
    "                        # Prefer original format over epodoc (cleaner formatting)\n",
    "                        if data_format == 'original' or len(seen_applicants) == 0:\n",
    "                            extracted['applicants'].append(clean_name)\n",
    "                            seen_applicants.add(clean_name)\n",
    "        \n",
    "        # Extract inventors (prefer original format, avoid duplicates)\n",
    "        inventors_section = parties.get('inventors', {})\n",
    "        inventors = inventors_section.get('inventor', [])\n",
    "        if not isinstance(inventors, list):\n",
    "            inventors = [inventors]\n",
    "        \n",
    "        seen_inventors = set()\n",
    "        # First pass: collect original format inventors\n",
    "        for inventor in inventors:\n",
    "            if isinstance(inventor, dict):\n",
    "                data_format = inventor.get('@data-format', '')\n",
    "                if data_format == 'original':\n",
    "                    name_obj = inventor.get('inventor-name', {})\n",
    "                    if isinstance(name_obj, dict):\n",
    "                        name = name_obj.get('name', {}).get('$', '')\n",
    "                        clean_name = name.strip().rstrip(',')  # Remove trailing comma\n",
    "                        if clean_name and clean_name not in seen_inventors:\n",
    "                            extracted['inventors'].append(clean_name)\n",
    "                            seen_inventors.add(clean_name)\n",
    "        \n",
    "        # If no original format found, use epodoc format\n",
    "        if not extracted['inventors']:\n",
    "            for inventor in inventors:\n",
    "                if isinstance(inventor, dict):\n",
    "                    data_format = inventor.get('@data-format', '')\n",
    "                    if data_format == 'epodoc':\n",
    "                        name_obj = inventor.get('inventor-name', {})\n",
    "                        if isinstance(name_obj, dict):\n",
    "                            name = name_obj.get('name', {}).get('$', '')\n",
    "                            clean_name = name.strip()\n",
    "                            if clean_name and clean_name not in seen_inventors:\n",
    "                                extracted['inventors'].append(clean_name)\n",
    "                                seen_inventors.add(clean_name)\n",
    "        \n",
    "        # Extract priority claims\n",
    "        priority_claims_section = biblio.get('priority-claims', {})\n",
    "        priority_claims = priority_claims_section.get('priority-claim', [])\n",
    "        if not isinstance(priority_claims, list):\n",
    "            priority_claims = [priority_claims]\n",
    "        \n",
    "        for priority in priority_claims:\n",
    "            if isinstance(priority, dict):\n",
    "                doc_ids = priority.get('document-id', [])\n",
    "                if isinstance(doc_ids, list):\n",
    "                    for doc_id in doc_ids:\n",
    "                        if doc_id.get('@document-id-type') == 'original':\n",
    "                            doc_num = doc_id.get('doc-number', {}).get('$', '')\n",
    "                            date = priority.get('document-id', [{}])[0].get('date', {}).get('$', '')\n",
    "                            if not date:  # Get date from epodoc format\n",
    "                                for did in doc_ids:\n",
    "                                    if did.get('@document-id-type') == 'epodoc':\n",
    "                                        date = did.get('date', {}).get('$', '')\n",
    "                                        break\n",
    "                            \n",
    "                            if doc_num and date:\n",
    "                                # Format the priority claim (country prefix + number + date)\n",
    "                                if doc_num.startswith('102017'):  # German application\n",
    "                                    priority_claim = f\"DE{doc_num}A·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                elif doc_num.startswith('EP2018'):  # EP PCT application\n",
    "                                    priority_claim = f\"{doc_num}W·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                else:\n",
    "                                    priority_claim = f\"{doc_num}·{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "                                extracted['priority_claims'].append(priority_claim)\n",
    "                            break\n",
    "        \n",
    "        # Extract IPC classifications\n",
    "        ipc_section = biblio.get('classifications-ipcr', {})\n",
    "        ipc_classifications = ipc_section.get('classification-ipcr', [])\n",
    "        if not isinstance(ipc_classifications, list):\n",
    "            ipc_classifications = [ipc_classifications]\n",
    "        \n",
    "        for ipc in ipc_classifications:\n",
    "            if isinstance(ipc, dict):\n",
    "                text_obj = ipc.get('text', {})\n",
    "                if isinstance(text_obj, dict):\n",
    "                    ipc_text = text_obj.get('$', '')\n",
    "                    if ipc_text:\n",
    "                        # Clean up IPC text: \"G01B   5/    00            A I\" → \"G01B5/00\"\n",
    "                        parts = ipc_text.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            clean_ipc = f\"{parts[0]}{parts[1]}\"\n",
    "                            extracted['ipc_classes'].append(clean_ipc)\n",
    "        \n",
    "        # Extract CPC classifications (in patent-classifications section)\n",
    "        patent_classifications = biblio.get('patent-classifications', {})\n",
    "        cpc_classifications = patent_classifications.get('patent-classification', [])\n",
    "        if not isinstance(cpc_classifications, list):\n",
    "            cpc_classifications = [cpc_classifications]\n",
    "        \n",
    "        for cpc in cpc_classifications:\n",
    "            if isinstance(cpc, dict):\n",
    "                scheme = cpc.get('classification-scheme', {})\n",
    "                if scheme.get('@scheme') == 'CPCI':  # CPC classification\n",
    "                    section = cpc.get('section', {}).get('$', '')\n",
    "                    class_code = cpc.get('class', {}).get('$', '')\n",
    "                    subclass = cpc.get('subclass', {}).get('$', '')\n",
    "                    main_group = cpc.get('main-group', {}).get('$', '')\n",
    "                    subgroup = cpc.get('subgroup', {}).get('$', '')\n",
    "                    office = cpc.get('generating-office', {}).get('$', '')\n",
    "                    \n",
    "                    if all([section, class_code, subclass, main_group, subgroup]):\n",
    "                        cpc_code = f\"{section}{class_code}{subclass}{main_group}/{subgroup} ({office})\"\n",
    "                        extracted['cpc_classes'].append(cpc_code)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during extraction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "print(\"✅ Patent data extraction function ready for accurate analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Selected University: University of Applied Sciences Saarbrücken\n",
      "📊 Number of granted patents: 2\n",
      "📅 Filing period: 2009 to 2018\n",
      "\n",
      "📋 Patent portfolio overview:\n",
      "  - EP09735811A (2009) - Other\n",
      "  - EP18826058A (2018) - Other\n",
      "\n",
      "🔬 Starting comprehensive priority and applicant analysis...\n",
      "💡 This demonstrates the methodology that can be scaled to larger university portfolios\n"
     ]
    }
   ],
   "source": [
    "# Select university with smallest patent portfolio for demonstration\n",
    "# This approach ensures manageable testing while demonstrating the methodology for patent searchers\n",
    "\n",
    "# Identify universities by patent count (smallest first)\n",
    "uni_counts = granted_patents.groupby('University').size().sort_values()\n",
    "smallest_uni = uni_counts.index[0]\n",
    "smallest_uni_patents = granted_patents[granted_patents['University'] == smallest_uni]\n",
    "\n",
    "print(f\"🎯 Selected University: {smallest_uni}\")\n",
    "print(f\"📊 Number of granted patents: {len(smallest_uni_patents)}\")\n",
    "print(f\"📅 Filing period: {smallest_uni_patents['Filing_Year'].min()} to {smallest_uni_patents['Filing_Year'].max()}\")\n",
    "\n",
    "print(f\"\\n📋 Patent portfolio overview:\")\n",
    "for idx, row in smallest_uni_patents.iterrows():\n",
    "    print(f\"  - {row['EP_Patent_Number']} ({row['Filing_Year']}) - {row['Technical_field']}\")\n",
    "\n",
    "print(f\"\\n🔬 Starting comprehensive priority and applicant analysis...\")\n",
    "print(f\"💡 This demonstrates the methodology that can be scaled to larger university portfolios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Analyzing EP09735811A...\n",
      "  ✅ Data successfully retrieved from EPO OPS\n",
      "  📋 Title: FILM RESISTOR WITH A CONSTANT TEMPERATURE COEFFICIENT AND PR...\n",
      "  👥 Applicants found: 5\n",
      "  🔬 Inventors identified: 12\n",
      "  🎯 Priority claims: 3\n",
      "  📚 IPC classifications: 6\n",
      "  📚 CPC classifications: 16\n",
      "    🎯 Priority: 102008022607·2008-04-24\n",
      "    🎯 Priority: 102009011353·2009-03-05\n",
      "    🎯 Priority: EP2009002530·2009-04-06\n",
      "    👤 Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES [DE]\n",
      "    👤 Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES,\n",
      "    👤 Applicant: SIEGERT TFT GMBH,\n",
      "    👤 Applicant: Hochschule für Technik und Wirtschaft des Saarlandes,\n",
      "    👤 Applicant: Siegert TFT GmbH\n",
      "    📚 IPC: G01L1/\n",
      "    📚 IPC: G01L1/\n",
      "\n",
      "🔍 Analyzing EP18826058A...\n",
      "  ✅ Data successfully retrieved from EPO OPS\n",
      "  📋 Title: STRAIN GAUGE COMPRISING A FLEXIBLE SUBSTRATE AND A RESISTANC...\n",
      "  👥 Applicants found: 2\n",
      "  🔬 Inventors identified: 4\n",
      "  🎯 Priority claims: 2\n",
      "  📚 IPC classifications: 3\n",
      "  📚 CPC classifications: 1\n",
      "    🎯 Priority: DE102017223831A·2017-12-28\n",
      "    🎯 Priority: EP2018086559W·2018-12-21\n",
      "    👤 Applicant: HOCHSCHULE FUER TECHNIK UND WIRTSCH DES SAARLANDES [DE]\n",
      "    👤 Applicant: Hochschule für Technik und Wirtschaft des Saarlandes\n",
      "    📚 IPC: G01B5/\n",
      "    📚 IPC: G01B7/\n",
      "\n",
      "📊 PATENT ANALYSIS COMPLETE\n",
      "============================================================\n",
      "✅ Successfully processed: 2/2 patents\n",
      "👥 Total unique applicants discovered: 7\n",
      "🔍 This reveals complete applicant landscape beyond DeepTechFinder data\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and analyze bibliographic data for each patent\n",
    "# This demonstrates how patent searchers can enrich DeepTechFinder data with comprehensive EPO OPS information\n",
    "\n",
    "priority_results = []\n",
    "all_applicants = set()\n",
    "\n",
    "for idx, row in smallest_uni_patents.iterrows():\n",
    "    patent_number = row['EP_Patent_Number']\n",
    "    print(f\"\\n🔍 Analyzing {patent_number}...\")\n",
    "    \n",
    "    # Retrieve bibliographic data from EPO OPS\n",
    "    biblio_data = ops_client.get_application_biblio(patent_number)\n",
    "    \n",
    "    if biblio_data:\n",
    "        # Extract comprehensive patent information\n",
    "        extracted = extract_patent_data(biblio_data)\n",
    "        \n",
    "        print(f\"  ✅ Data successfully retrieved from EPO OPS\")\n",
    "        print(f\"  📋 Title: {extracted['title'][:60] if extracted['title'] else 'N/A'}...\")\n",
    "        print(f\"  👥 Applicants found: {len(extracted['applicants'])}\")\n",
    "        print(f\"  🔬 Inventors identified: {len(extracted['inventors'])}\")\n",
    "        print(f\"  🎯 Priority claims: {len(extracted['priority_claims'])}\")\n",
    "        print(f\"  📚 IPC classifications: {len(extracted['ipc_classes'])}\")\n",
    "        print(f\"  📚 CPC classifications: {len(extracted['cpc_classes'])}\")\n",
    "        \n",
    "        # Collect all unique applicants (critical for identifying collaborations)\n",
    "        for applicant in extracted['applicants']:\n",
    "            all_applicants.add(applicant)\n",
    "        \n",
    "        # Store complete results for analysis\n",
    "        result = {\n",
    "            'ep_patent': patent_number,\n",
    "            'filing_year': row['Filing_Year'],\n",
    "            'technical_field': row['Technical_field'],\n",
    "            'title': extracted['title'],\n",
    "            'applicants': extracted['applicants'],\n",
    "            'inventors': extracted['inventors'],\n",
    "            'priority_claims': extracted['priority_claims'],\n",
    "            'ipc_classes': extracted['ipc_classes'],\n",
    "            'cpc_classes': extracted['cpc_classes'],\n",
    "            'application_number': extracted['application_number'],\n",
    "            'filing_date': extracted['filing_date']\n",
    "        }\n",
    "        priority_results.append(result)\n",
    "        \n",
    "        # Display key findings for patent searchers\n",
    "        if extracted['priority_claims']:\n",
    "            for priority in extracted['priority_claims']:\n",
    "                print(f\"    🎯 Priority: {priority}\")\n",
    "        else:\n",
    "            print(f\"    🎯 Priority: None found (likely original filing)\")\n",
    "        \n",
    "        # Show all applicants (reveals collaborations beyond university)\n",
    "        for applicant in extracted['applicants']:\n",
    "            print(f\"    👤 Applicant: {applicant}\")\n",
    "        \n",
    "        # Display technology classifications\n",
    "        for ipc in extracted['ipc_classes'][:2]:  # Show first 2 IPC codes\n",
    "            print(f\"    📚 IPC: {ipc}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"  ❌ Patent not found in EPO OPS database (may be older patent with limited coverage)\")\n",
    "    \n",
    "    # Rate limiting to respect EPO OPS usage policies\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\n📊 PATENT ANALYSIS COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"✅ Successfully processed: {len(priority_results)}/{len(smallest_uni_patents)} patents\")\n",
    "print(f\"👥 Total unique applicants discovered: {len(all_applicants)}\")\n",
    "print(f\"🔍 This reveals complete applicant landscape beyond DeepTechFinder data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 CORE FINDINGS:\n",
      "University: University of Applied Sciences Saarbrücken\n",
      "Patents analyzed: 2\n",
      "\n",
      "👥 ALL APPLICANTS DISCOVERED:\n",
      "  1. HTW SAARLAND\n",
      "  2. SIEGERT TFT\n",
      "\n",
      "🔗 PRIORITY PATENT FAMILIES:\n",
      "  102008022607 → EP09735811A | Applicants: SIEGERT TFT, HTW SAARLAND\n",
      "  DE102017223831A → EP18826058A | Applicants: HTW SAARLAND\n",
      "\n",
      "🤝 INDUSTRY COLLABORATIONS:\n",
      "  • SIEGERT TFT\n",
      "\n",
      "💾 Exported: ./output/University_of_Applied_Sciences_Saarbrücken_core_analysis.csv\n",
      "\n",
      "✅ Analysis complete - 2 total applicants discovered\n"
     ]
    }
   ],
   "source": [
    "# Core applicant analysis: Find all collaborators and priority patent insights\n",
    "# Focus: University + discovered collaborators + priority patent family analysis\n",
    "\n",
    "def normalize_applicant_name(name):\n",
    "    \"\"\"Normalize applicant names to combine similar variations\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    \n",
    "    normalized = name.upper().strip().split('[')[0].strip().rstrip(',').strip()\n",
    "    \n",
    "    # University normalization patterns\n",
    "    replacements = {\n",
    "        'HOCHSCHULE FUER TECHNIK UND WIRTSCHAFT DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'HOCHSCHULE FÜR TECHNIK UND WIRTSCHAFT DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'HOCHSCHULE FUER TECHNIK UND WIRTSCH DES SAARLANDES': 'HTW SAARLAND',\n",
    "        'SIEGERT TFT GMBH': 'SIEGERT TFT',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        if old in normalized:\n",
    "            return new\n",
    "    \n",
    "    return normalized.strip()\n",
    "\n",
    "# Extract normalized applicants from all patents\n",
    "all_normalized_applicants = set()\n",
    "priority_insights = []\n",
    "\n",
    "for result in priority_results:\n",
    "    # Normalize applicants\n",
    "    normalized_applicants = list(set([normalize_applicant_name(app) for app in result['applicants'] if normalize_applicant_name(app)]))\n",
    "    all_normalized_applicants.update(normalized_applicants)\n",
    "    \n",
    "    # Priority analysis\n",
    "    if result['priority_claims']:\n",
    "        first_priority = result['priority_claims'][0]\n",
    "        priority_number = first_priority.split('·')[0] if '·' in first_priority else first_priority\n",
    "        priority_insights.append({\n",
    "            'ep_patent': result['ep_patent'], \n",
    "            'priority': priority_number,\n",
    "            'applicants': normalized_applicants\n",
    "        })\n",
    "\n",
    "# Core Results\n",
    "print(f\"🎯 CORE FINDINGS:\")\n",
    "print(f\"University: {smallest_uni}\")\n",
    "print(f\"Patents analyzed: {len(priority_results)}\")\n",
    "\n",
    "print(f\"\\n👥 ALL APPLICANTS DISCOVERED:\")\n",
    "for i, applicant in enumerate(sorted(all_normalized_applicants), 1):\n",
    "    print(f\"  {i}. {applicant}\")\n",
    "\n",
    "print(f\"\\n🔗 PRIORITY PATENT FAMILIES:\")\n",
    "for insight in priority_insights:\n",
    "    print(f\"  {insight['priority']} → {insight['ep_patent']} | Applicants: {', '.join(insight['applicants'])}\")\n",
    "\n",
    "# Identify collaborations (non-university applicants)\n",
    "university_terms = ['university', 'universität', 'hochschule', 'institut', 'htw', 'kit', 'tu ', 'rwth']\n",
    "collaborators = [app for app in all_normalized_applicants \n",
    "                if not any(term in app.lower() for term in university_terms)]\n",
    "\n",
    "if collaborators:\n",
    "    print(f\"\\n🤝 INDUSTRY COLLABORATIONS:\")\n",
    "    for collab in sorted(collaborators):\n",
    "        print(f\"  • {collab}\")\n",
    "\n",
    "# Export core results\n",
    "if priority_results:\n",
    "    # Add normalized data\n",
    "    for result in priority_results:\n",
    "        result['normalized_applicants'] = [normalize_applicant_name(app) for app in result['applicants'] if normalize_applicant_name(app)]\n",
    "    \n",
    "    results_df = pd.DataFrame(priority_results)\n",
    "    output_file = f\"./output/{smallest_uni.replace(' ', '_').replace('/', '_')}_core_analysis.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n💾 Exported: {output_file}\")\n",
    "\n",
    "print(f\"\\n✅ Analysis complete - {len(all_normalized_applicants)} total applicants discovered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps for Patent Searchers\n",
    "\n",
    "### Scaling to Full Datasets\n",
    "- **Batch Processing**: Modify the analysis loop to process larger university portfolios\n",
    "- **Rate Limiting**: Implement proper delays to respect EPO OPS usage limits (current: 2 seconds between requests)\n",
    "- **Error Handling**: Add retry logic for temporary API failures\n",
    "- **Progress Tracking**: Save intermediate results to prevent data loss during long runs\n",
    "\n",
    "### Advanced Analysis Opportunities\n",
    "- **Family Mapping**: Use priority claims to build complete patent family trees\n",
    "- **Collaboration Networks**: Analyze co-applicant patterns across universities\n",
    "- **Technology Landscapes**: Group patents by IPC/CPC codes for technology mapping\n",
    "- **Timeline Analysis**: Track filing strategies and priority patterns over time\n",
    "\n",
    "### Integration with Patent Search Workflows\n",
    "- **Due Diligence Enhancement**: Supplement freedom-to-operate searches with complete applicant data\n",
    "- **Competitive Intelligence**: Identify university-industry partnerships and licensing patterns\n",
    "- **Prior Art Searching**: Use enhanced classification data for more precise search strategies\n",
    "- **Portfolio Analysis**: Compare university patent strategies and collaboration approaches\n",
    "\n",
    "### Data Export Options\n",
    "- **CSV Files**: For integration with Excel and database systems\n",
    "- **JSON Format**: For web applications and API integrations  \n",
    "- **Patent Family Reports**: Structured reports showing priority relationships\n",
    "- **Collaboration Maps**: Network visualizations of applicant relationships\n",
    "\n",
    "This methodology provides patent information professionals with powerful tools to enrich and analyze university patent portfolios beyond standard database searches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
